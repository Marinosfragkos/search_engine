{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine\n",
    " Εισαγωγή απαραίτητων βιβλιοθηκών για εξαγωγή δεδομένων από τον ιστό, προεπεξεργασία κειμένου και αξιολόγηση αλγορίθμων αναζήτησης:\n",
    " - requests: Επιτρέπει την αποστολή HTTP αιτημάτων για την απόκτηση περιεχομένου ιστού.\n",
    " - BeautifulSoup: Αναλύει HTML περιεχόμενο για την εξαγωγή σχετικών δεδομένων.\n",
    " - json: Χρησιμοποιείται για τη διαχείριση δεδομένων σε μορφή JSON για δομημένη είσοδο και έξοδο.\n",
    " - nltk: Βιβλιοθήκη για επεξεργασία φυσικής γλώσσας, που χρησιμοποιείται για τον τεμαχισμό λέξεων, την αφαίρεση άχρηστων λέξεων, τη μετοχή και τη λεμματοποίηση.\n",
    " - sklearn: Παρέχει εργαλεία μηχανικής μάθησης, όπως η μεθοδολογία TF-IDF και ο υπολογισμός ομοιότητας cosine.\n",
    " - rank_bm25: Υλοποιεί τον αλγόριθμο BM25 για την κατάταξη εγγράφων βάσει της σχετικότητάς τους.\n",
    " - numpy: Προσφέρει αποδοτικές αριθμητικές λειτουργίες για επεξεργασία δεδομένων και υπολογισμούς σε πίνακες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc555124",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marinosfrangos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marinosfrangos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/marinosfrangos/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marinosfrangos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405903eb",
   "metadata": {},
   "source": [
    "Ερώτημα 1\n",
    "Η συνάρτηση scrape_polynoe() εξάγει δεδομένα από τη σελίδα του αποθετηρίου Polynoe. \n",
    "Συγκεκριμένα, αντλεί πληροφορίες για τον τίτλο, τον συγγραφέα, την ημερομηνία και την περίληψη κάθε εγγράφου.\n",
    "Τα δεδομένα συλλέγονται από τα στοιχεία HTML της σελίδας, οργανώνονται σε λίστα και αποθηκεύονται σε αρχείο JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b972178d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def scrape_polynoe():\n",
    "    url = 'https://polynoe.lib.uniwa.gr/xmlui/browse?type=dateissued'\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    descriptions = soup.find_all('div', class_='artifact-description')\n",
    "    data = []\n",
    "\n",
    "    for desc in descriptions:\n",
    "        title = desc.find('h4', class_='artifact-title').text.strip()\n",
    "        author = desc.find('span', class_='author h4').text.strip()\n",
    "        date = desc.find('span', class_='date').text.strip()\n",
    "        abstract = desc.find('div', class_='artifact-abstract').text.strip()\n",
    "        data.append([title, author, date, abstract])\n",
    "\n",
    "    with open('data.json', 'w', encoding='utf8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ca590",
   "metadata": {},
   "source": [
    "Ερώτημα 2\n",
    "Η συνάρτηση preprocess_text() προεπεξεργάζεται τα δεδομένα που έχουν εξαχθεί από το αρχείο JSON. \n",
    "Αφαιρεί άχρηστες λέξεις (stop words), διατηρεί μόνο αλφαβητικούς χαρακτήρες, και εφαρμόζει διαδικασίες μετοχής (stemming) και λεμματοποίησης (lemmatization).\n",
    "Οι προεπεξεργασμένες πληροφορίες (τίτλος, συγγραφέας, περίληψη) αποθηκεύονται σε νέο αρχείο JSON για περαιτέρω ανάλυση ή χρήση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cced20b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_text():\n",
    "\n",
    "    stop_words = set(stopwords.words('greek'))  # Define stop_words\n",
    "    stemmer = PorterStemmer()  # Define stemmer\n",
    "    lemmatizer = WordNetLemmatizer()  # Define lemmatizer\n",
    "\n",
    "    with open('data.json', 'r', encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    for entry in data:\n",
    "        title_tokens = word_tokenize(entry[0])\n",
    "        title_tokens = [word.lower() for word in title_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "        title_tokens = [stemmer.stem(word) for word in title_tokens]\n",
    "        title_tokens = [lemmatizer.lemmatize(word) for word in title_tokens]\n",
    "\n",
    "        author_tokens = word_tokenize(entry[1])\n",
    "        author_tokens = [word.lower() for word in author_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "        author_tokens = [stemmer.stem(word) for word in author_tokens]\n",
    "        author_tokens = [lemmatizer.lemmatize(word) for word in author_tokens]\n",
    "\n",
    "        abstract_tokens = word_tokenize(entry[3])\n",
    "        abstract_tokens = [word.lower() for word in abstract_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "        abstract_tokens = [stemmer.stem(word) for word in abstract_tokens]\n",
    "        abstract_tokens = [lemmatizer.lemmatize(word) for word in abstract_tokens]\n",
    "\n",
    "        processed_data.append({\n",
    "            'title': title_tokens,\n",
    "            'author': author_tokens,\n",
    "            'date': entry[2],\n",
    "            'abstract': abstract_tokens\n",
    "        })\n",
    "\n",
    "    with open('processed_data.json', 'w', encoding='utf8') as f:\n",
    "        json.dump(processed_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24601e76",
   "metadata": {},
   "source": [
    "Ερώτημα 3\n",
    "Η συνάρτηση create_inverted_index() δημιουργεί ένα αντεστραμμένο ευρετήριο (inverted index) από τα δεδομένα του αρχείου JSON που περιέχουν τις προεπεξεργασμένες πληροφορίες.\n",
    "Κάθε λέξη από την περίληψη των εγγράφων αντιστοιχεί σε ένα σύνολο από έγγραφα (δείκτες) όπου εμφανίζεται.\n",
    "Το αντίστροφο ευρετήριο αποθηκεύεται σε νέο αρχείο JSON για να χρησιμοποιηθεί σε μελλοντική αναζήτηση ή ανάλυση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64f8fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index():\n",
    "    with open('processed_data.json', 'r', encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "    inverted_index = defaultdict(set)\n",
    "    for i, entry in enumerate(data):\n",
    "        for word in entry['abstract']:\n",
    "            inverted_index[word].add(i)\n",
    "    inverted_index = {k: list(v) for k, v in inverted_index.items()}\n",
    "    with open('inverted_index.json', 'w', encoding='utf8') as f:\n",
    "        json.dump(inverted_index, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6e948",
   "metadata": {},
   "source": [
    "Ερώτημα 4\n",
    "Η συνάρτηση search() επιτρέπει στον χρήστη να επιλέξει έναν αλγόριθμο αναζήτησης για την εκτέλεση της αναζήτησης με βάση το δεδομένο ερώτημα.\n",
    "Παρουσιάζει τρεις επιλογές: Boolean Retrieval, Vector Space Model και Okapi BM25.\n",
    "Ανάλογα με την επιλογή του χρήστη, καλεί την αντίστοιχη συνάρτηση αναζήτησης και εμφανίζει τα αποτελέσματα.\n",
    "Αν η επιλογή είναι μη έγκυρη, εμφανίζει μήνυμα λάθους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c7ecf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(search_query):\n",
    "    print(\"Please choose an algorithm:\")\n",
    "    print(\"1. Boolean Retrieval\")\n",
    "    print(\"2. Vector Space Model\")\n",
    "    print(\"3. Okapi BM25\")\n",
    "    choice = int(input(\"Enter your choice (1-3): \"))\n",
    "\n",
    "    if choice == 1:\n",
    "        print(boolean_retrieval(search_query))\n",
    "    elif choice == 2:\n",
    "        print(ranking(search_query,'vectorspacemodel'))\n",
    "    elif choice == 3:\n",
    "        print(ranking(search_query,'okapibm25'))\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter a number between 1 and 3.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6bc9a3",
   "metadata": {},
   "source": [
    "Η συνάρτηση boolean_retrieval() εκτελεί αναζήτηση βάσει του μοντέλου Boolean, χρησιμοποιώντας τον αντεστραμμένο ευρετήριο.\n",
    "Αρχικά, επεξεργάζεται το ερώτημα και φορτώνει τον αντίστροφο ευρετήριο από το αρχείο JSON.\n",
    "Στη συνέχεια, εφαρμόζει τους λογικούς τελεστές 'AND', 'OR' και 'NOT' για να εντοπίσει τα σχετικά έγγραφα.\n",
    "Η συνάρτηση επιστρέφει τη λίστα των εγγράφων που πληρούν τις συνθήκες του ερωτήματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1067c583",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def boolean_retrieval(query):\n",
    "    query = query_processing(query)\n",
    "\n",
    "    # Load the inverted index from the JSON file\n",
    "    with open('inverted_index.json', 'r', encoding='utf8') as f:\n",
    "        inverted_index = json.load(f)\n",
    "\n",
    "    # Initialize the set of documents\n",
    "    docs = set(inverted_index.get(query[0], []))\n",
    "\n",
    "    # Apply Boolean operators\n",
    "    for i in range(1, len(query), 2):\n",
    "        operator = query[i]\n",
    "        word = query[i+1]\n",
    "\n",
    "        if operator.lower() == 'and':\n",
    "            docs &= set(inverted_index.get(word, []))\n",
    "        elif operator.lower() == 'or':\n",
    "            docs |= set(inverted_index.get(word, []))\n",
    "        elif operator.lower() == 'not':\n",
    "            docs -= set(inverted_index.get(word, []))\n",
    "\n",
    "    return list(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf11f6",
   "metadata": {},
   "source": [
    "Η συνάρτηση vector_space_model() εκτελεί αναζήτηση χρησιμοποιώντας το μοντέλο του διανύσματος χώρου (Vector Space Model) με βάση την ομοιότητα cosine.\n",
    "Αρχικά, φορτώνει τα προεπεξεργασμένα έγγραφα και το ερώτημα, το οποίο αναλύεται σε λέξεις (tokenization).\n",
    "Υπολογίζει τη συχνότητα εμφάνισης λέξεων (TF) και την αντίστοιχη συχνότητα αντεστραμμένης εμφάνισης (IDF) για όλα τα έγγραφα και το ερώτημα.\n",
    "Στη συνέχεια, υπολογίζει την ομοιότητα cosine ανάμεσα στο ερώτημα και τα έγγραφα, και τα ταξινομεί κατά σειρά ομοιότητας.\n",
    "Η συνάρτηση επιστρέφει τα έγγραφα ταξινομημένα με βάση την ομοιότητά τους με το ερώτημα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "787dd5f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def vector_space_model(query):\n",
    "    # Load preprocessed documents from JSON file\n",
    "    with open('processed_data.json', 'r', encoding='utf8') as f:\n",
    "        documents = json.load(f)\n",
    "\n",
    "    # Tokenize the query\n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    # Convert tokenized documents to text\n",
    "    preprocessed_documents = [' '.join(doc['title'] + doc['author'] + doc['abstract'] + [doc['date']]) for doc in documents]  # Combine all fields\n",
    "    preprocessed_query = ' '.join(tokenized_query)\n",
    "\n",
    "    # Create a TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "    # Transform the query into a TF-IDF vector\n",
    "    query_vector = tfidf_vectorizer.transform([preprocessed_query])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "    # Rank documents by similarity\n",
    "    results = [(documents[i], cosine_similarities[0][i]) for i in range(len(documents))]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the top 5 ranked documents\n",
    "    # for doc, similarity in results[:5]:  \n",
    "    #     print(f\"Similarity: {similarity:.2f}\\nTitle: {' '.join(doc['title'])}\\nAuthor: {' '.join(doc['author'])}\\nDate: {doc['date']}\\nAbstract: {' '.join(doc['abstract'])}\\n\")  # Print all fields\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfed3e",
   "metadata": {},
   "source": [
    "Η συνάρτηση okapibm25() εκτελεί αναζήτηση χρησιμοποιώντας τον αλγόριθμο Okapi BM25, ο οποίος αξιολογεί τη συσχέτιση των εγγράφων με το ερώτημα.\n",
    "Αρχικά, φορτώνει τα προεπεξεργασμένα έγγραφα και το ερώτημα, το οποίο διασπάται σε λέξεις (tokenization).\n",
    "Στη συνέχεια, ο αλγόριθμος BM25 υπολογίζει τις βαθμολογίες ομοιότητας για κάθε έγγραφο και επιστρέφει τα καλύτερα αποτελέσματα ταξινομημένα κατά τις βαθμολογίες τους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e9b012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapibm25(query):\n",
    "    # Load preprocessed documents from JSON file\n",
    "    with open('processed_data.json', 'r', encoding='utf8') as f:\n",
    "        documents = json.load(f)\n",
    "\n",
    "    # Tokenize the query\n",
    "    tokenized_query = query.split(\" \")\n",
    "\n",
    "    # Convert tokenized documents to text\n",
    "    preprocessed_documents = [' '.join(doc['title'] + doc['author'] + doc['abstract'] + [doc['date']]) for doc in documents]  # Combine all fields\n",
    "\n",
    "    # Initialize BM25Okapi model\n",
    "    bm25 = BM25Okapi([doc.split(\" \") for doc in preprocessed_documents])\n",
    "\n",
    "    # Get scores for each document\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    # Get the indices of the top documents\n",
    "    top_indices = bm25.get_top_n(tokenized_query, range(len(preprocessed_documents)), n=5)\n",
    "\n",
    "    # # Print the details of the top documents\n",
    "    # for index in top_indices:\n",
    "    #     print(f\"Similarity Score: {doc_scores[index]}\")\n",
    "    #     print(f\"Title: {documents[index]['title']}\")\n",
    "    #     print(f\"Author: {documents[index]['author']}\")\n",
    "    #     print(f\"Abstract: {documents[index]['abstract']}\")\n",
    "    #     print(f\"Date: {documents[index]['date']}\")\n",
    "    #     print(\"\\n\")\n",
    "    results = [(documents[i], doc_scores[i]) for i in top_indices]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dadc922",
   "metadata": {},
   "source": [
    "Φιλτράρισμα Αποτελεσμάτων \n",
    "def filter_results(criteria, value):\n",
    "    # Άνοιγμα του αρχείου με τα επεξεργασμένα δεδομένα\n",
    "    with open('processed_data.json', 'r', encoding='utf8') as f:\n",
    "        data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d0407a",
   "metadata": {},
   "source": [
    "    # Δημιουργία μιας λίστας με τα έγγραφα που πληρούν το κριτήριο\n",
    "    filtered_data = [doc for doc in data if doc.get(criteria) == value]\n",
    "        print(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a32c11",
   "metadata": {},
   "source": [
    "    # Επιστροφή της λίστας με τα φιλτραρισμένα δεδομένα\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917489d4",
   "metadata": {},
   "source": [
    "Επεξεργασία ερωτήματος (Query Processing)\n",
    "Η συνάρτηση query_processing() προεπεξεργάζεται το ερώτημα αναζήτησης με σκοπό να το καταστήσει πιο αποδοτικό για την αναζήτηση.\n",
    "Αρχικά, διαγράφονται οι άχρηστες λέξεις (stop words), έπειτα οι λέξεις του ερωτήματος μετατρέπονται σε μικρά γράμματα.\n",
    "Στη συνέχεια, εφαρμόζεται το stemming και η λεμματοποίηση για να μειωθούν οι λέξεις στην βασική τους μορφή.\n",
    "Τέλος, επιστρέφει τη λίστα με τις επεξεργασμένες λέξεις του ερωτήματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5effac25",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def query_processing(query):\n",
    "\n",
    "    stop_words = set(stopwords.words('greek'))  # Define stop_words\n",
    "    stemmer = PorterStemmer()  # Define stemmer\n",
    "    lemmatizer = WordNetLemmatizer()  # Define lemmatizer\n",
    "\n",
    "    query_tokens = word_tokenize(query)\n",
    "    query_tokens = [word.lower() for word in query_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    query_tokens = [stemmer.stem(word) for word in query_tokens]\n",
    "    query_tokens = [lemmatizer.lemmatize(word) for word in query_tokens]\n",
    "    \n",
    "    return query_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a1629",
   "metadata": {},
   "source": [
    "Κατάταξη αποτελεσμάτων (Ranking)\n",
    "Η συνάρτηση ranking() επιλέγει και εφαρμόζει έναν αλγόριθμο ταξινόμησης για να επιστρέψει τα πιο σχετικά αποτελέσματα για το δοθέν ερώτημα.\n",
    "Ανάλογα με την τιμή του παραμέτρου `ranking_algorithm`, καλείται είτε ο αλγόριθμος του Vector Space Model είτε ο αλγόριθμος Okapi BM25.\n",
    "Στη συνέχεια, εκτυπώνει τα αποτελέσματα ταξινομημένα κατά τη σχετικότητα τους, εμφανίζοντας τις βασικές πληροφορίες (τίτλος, συγγραφέας, ημερομηνία, περίληψη)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea757e82",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ranking(query, ranking_algorithm):\n",
    "    if ranking_algorithm == 'vectorspacemodel':\n",
    "        results = vector_space_model(query)\n",
    "    elif ranking_algorithm == 'okapibm25':\n",
    "        results = okapibm25(query)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported ranking algorithm\")\n",
    "    for doc, similarity in results:\n",
    "        print(f\"Similarity: {similarity:.2f}\\nTitle: {' '.join(doc['title'])}\\nAuthor: {' '.join(doc['author'])}\\nDate: {doc['date']}\\nAbstract: {' '.join(doc['abstract'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3d854",
   "metadata": {},
   "source": [
    "Αυτή η συνάρτηση είναι η κύρια συνάρτηση του προγράμματος. \n",
    "Ξεκινά με την εξαγωγή των δεδομένων από τον ιστότοπο Polynoe (scrape_polynoe), \n",
    "την προεπεξεργασία των δεδομένων (preprocess_text), και τη δημιουργία του αντίστροφου ευρετηρίου (create_inverted_index). \n",
    "Στη συνέχεια, ζητά από τον χρήστη να εισάγει ένα ερώτημα αναζήτησης και καλεί τη συνάρτηση αναζήτησης (search) για να εμφανίσει τα αποτελέσματα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3b82d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose an algorithm:\n",
      "1. Boolean Retrieval\n",
      "2. Vector Space Model\n",
      "3. Okapi BM25\n",
      "Similarity: 1.04\n",
      "Title: ομότιμη συνεργατική αλληλεπίδραση ανθρώπου μηχανής\n",
      "Author: τουμανίδης λάζαρος\n",
      "Date: 2020-04\n",
      "Abstract: μια συνήθης πρακτική λύσεις μηχανικής μάθησης είναι συνεχής χρήση της ανθρώπινης ευφυίας σκοπό βελτίωση της ποιότητας αποδοτικότητάς τους αντικείμενο της παρούσας εργασίας είναι μελέτη τρόπων\n",
      "\n",
      "Similarity: 0.99\n",
      "Title: ανάλυση χρονοσειρών ηλεκτρομαγνητικών μετρήσεων δικτύου επίγειων σταθμών ανίχνευση υπογραφών καταστροφικών γεωφυσικών φαινομένων\n",
      "Author: πολίτης δημήτριος\n",
      "Date: 2020-04\n",
      "Abstract: παρούσα διπλωματική εργασία γίνεται αναφορά όλο εύρος της εκπόνησης της έρευνας όλη τη διάρκεια της φοίτησης πρόγραμμα μεταπτυχιακών σπουδών ηλεκτρικές ηλεκτρονικές επιστήμες μέσω έρευνας ερευνητικό\n",
      "\n",
      "Similarity: 0.92\n",
      "Title: αντιστάθμιση χρηματοοικονομικού κινδύνου ναυτιλία χρήση παραγώγων\n",
      "Author: κασίμης βρεττός\n",
      "Date: 2012-04-28\n",
      "Abstract: ορθή αποδοτική λειτουργία ή ανάπτυξη μίας επιχείρησης κρίνεται σκόπιμο μελετάται χρηματοοικονομικός της κίνδυνος χώρο της ναυτιλίας όπου κίνδυνοι ποικίλουν ανάλυση εκτίμηση\n",
      "\n",
      "Similarity: 0.91\n",
      "Title: πλατφόρμα καταχώρισης διαχείρισης βλαβών δήμου\n",
      "Author: δημητρόπουλος αναστάσιος\n",
      "Date: 2020-06-09\n",
      "Abstract: σκοπός της εργασίας είναι δημιουργία μια διαδικτυακής πλατφόρμας καταχώρησης διαχείριση τυχών βλαβών συμβάντων στα πλαίσια ενός δήμου καθώς επίσης της άμεσης αλληλεπίδρασης δημοτών\n",
      "\n",
      "Similarity: 0.88\n",
      "Title: βελτιστοποίηση τεχνικών εκπαίδευσης νευρωνικών δικτύων εμπρόσθιας τροφοδότησης επίλυση διαφορικών εξισώσεων\n",
      "Author: καλούτσα βασιλική\n",
      "Date: 2020-05\n",
      "Abstract: διαφορικές εξισώσεις αποτελούν μαθηματικά εργαλεία τη μοντελοποίηση πληθώρας προβλημάτων οποία περιέχουν ποσότητες μεταβάλλονται οποία συναντώνται πολλούς τομείς όπως αυτούς της μηχανικής της\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scrape_polynoe()\n",
    "    preprocess_text()\n",
    "    create_inverted_index()\n",
    "    search_query = input(\"Enter your search query: \")\n",
    "    #filters = input(\"Enter your filter: \")\n",
    "    search(search_query)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
